{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "케라스_창시자에게_배우는_딥러닝.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP7/GOBw0u+e5v3RpEKltvE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parkmina365/deep_learning_with_python/blob/main/CH_3%20%EC%8B%A0%EA%B2%BD%EB%A7%9D%20%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzgN853PCHyT"
      },
      "source": [
        "# 3장. 신경망 시작하기\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyeJYHDy1L5O"
      },
      "source": [
        "## 3.1 신경망의 구조\n",
        "- 층, 입력데이터와 타깃, 손실 함수, 옵티마이저\n",
        "- 연속된 층으로 구성된 네트워크가 입력 데이터를 예측으로 매핑. 손실 함수는 예측과 타깃을 비교하여 네트워크의 예측이 기댓값에 얼마나 잘 맞는지를 측정하는 손실 값을 만듦. 옵티마이저는 손실 값을 사용하여 네트워크 가중치를 업데이트함\n",
        "\n",
        "3.1.1 층: 딥러닝의 구성 단위\n",
        "- 신경망 핵심적인 데이터 구조. 층은 하나 이상의 텐서를 입력으로 받아 하나 이상의 텐서를 출력하는 데이터 처리 모듈. 어떤 종류의 층은 상태가 없지만 대부분의 경우 가중치라는 층의 상태를 가짐. 가중치는 확률적 경사 하강법에 의해 학습되는 하나 이상의 텐서이며 여기에 네트워크가 학습한 지식이 담겨 있음.\n",
        "- 층마다 적절한 텐서 포맷과 데이터 처리 방식이 다름. 예를 들어 (samples, features) 크기 의 2D 텐서가 저장된 간단한 벡터 데이터는 완전 연결 층(fully connected layer)이나 밀집 층(dense laver) 이라고도 불리는 밀집 연결 층(densely connected layer)에 의해 처리되는 경우가 많습니다(케라스에서는 Dense 클래스입니다)\n",
        "- (samples, timesteps, features) 크기의 3D 텐서로 저장된 시퀀스 데이터는 보통 LSTM 같은 순환 층(recurrent layer)에 의해 처리됨\n",
        "- 4D 텐서로 저장되 어 있는 이미지 데이터는 일반적으로 2D 합성곱 층(convolution layer)에 의해 처리됩니다(Conv2D\n",
        "클래스).\n",
        "- 플랫튼(Flatten), 풀링(Pooling), 드롭아웃(Dropout)층에는 학습되는 가중치가 없음. 보통 신경망의 가중치를 상태라고 표현하지는 않습니다. 순환 신경망의 셀(Cell) 출력은 셀의 상태라고 표현함\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3.1.2 모델: 층의 네트워크\\\n",
        "\n",
        "\n",
        "3.1.3 손실 함수와 옵티마이저: 학습 과정을 조절하는 열쇠\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evPj65Mq1Php"
      },
      "source": [
        "## 3.2 케라스 소개\n",
        "3.2.1 케라스, 텐서플로, 씨아노, CNTK\\\n",
        "3.2.2 케라스를 사용한 개발: 빠르게 둘러보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUufXQSi1GEl"
      },
      "source": [
        "## 3.3 딥러닝 컴퓨터 셋팅\n",
        "3.3.1 주피터 노트북: 딥러닝 실험을 위한 최적의 방법\\\n",
        "3.3.2 케라스 시작하기: 두 가지 방법\\\n",
        "3.3.3 클라우드에서 딥러닝 작업을 수행했을 때 장단점\\\n",
        "3.3.4 어떤 GPU 카드가 딥러닝에 최적일까?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78iPDjkd7Mnu"
      },
      "source": [
        "## 3.4 영화 리뷰 분류: 이진 분류 예제\n",
        "3.4.1 IMDB 데이터셋\\\n",
        "3.4.2 데이터 준비\\\n",
        "3.4.3 신경망 모델 만들기\\\n",
        "3.4.4 훈련 검증\\\n",
        "3.4.5 훈련된 모델로 새로운 데이터에 대해 예측하기\\\n",
        "3.4.6 추가 실험\\\n",
        "3.4.7 정리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "LHn4CZ_4-Q3W",
        "outputId": "c769d0e8-7f56-400d-9dd9-ce9e7f24efc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels)\\\n",
        " = imdb.load_data(num_words=10000)\n",
        " # num_words=10000: 훈련 데이터에서 자주 나타나는 단어 1만개만 사용"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "14573568/17464789 [========================>.....] - ETA: 0s"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-665169d48199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m  \u001b[0;31m# num_words=10000: 훈련 데이터에서 자주 나타나는 단어 1만개만 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path, num_words, skip_top, maxlen, seed, start_char, oov_char, index_from, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morigin_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'imdb.npz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mfile_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m       '69664113be75683a8fe16e3ed0ab59fda8886cb3cd7ada244f7d9544e4676b9f')\n\u001b[0m\u001b[1;32m    108\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unexpected-keyword-arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mchunk_read\u001b[0;34m(response, chunk_size, reporthook)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m           \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m           \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mdl_progress\u001b[0;34m(count, block_size, total_size)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mProgressTracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProgbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mProgressTracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'URL fetch failure on {}: {} -- {}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKsD3fC5_srC"
      },
      "source": [
        "# train_data 데이터 형태 파악\n",
        "print(train_data[:5])\n",
        "print(train_data[0])\n",
        "print(len(train_data[0]))\n",
        "print(train_data.shape)\n",
        "print()\n",
        "\n",
        "import numpy as np\n",
        "print(np.sort([max(sequence) for sequence in train_data]))\n",
        "### 인덱스는 9999를 넘지 않음(자주 등장하는 단어를 1만개로 제한했기에)\n",
        "\n",
        "print(train_labels[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTcpfQCPBRjZ"
      },
      "source": [
        "# 정수 시퀀스를 이진 행렬로 인코딩\n",
        "import numpy as np \n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] =1.  # 부동소숫점 넣어줘야함\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR9p1V5wCh-j"
      },
      "source": [
        "print(x_train[0])\n",
        "print(len(x_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "633Lm0f-DB26"
      },
      "source": [
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdhqROPXDWKg"
      },
      "source": [
        "# 모델 정의하기\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kInm8JuSEOXu"
      },
      "source": [
        "# 모델 컴파일하기\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giYhMKExEnQV"
      },
      "source": [
        "# 검증 세트 분리\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILn_YzVGFIlq"
      },
      "source": [
        "# 모델 훈련\n",
        "history = model.fit(partial_x_train, partial_y_train,\\\n",
        "                    epochs=20, batch_size=512,\\\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2mNnX-mFm0v"
      },
      "source": [
        "# 훈련과 검증 손실 그리기\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "epochs = range(1, len(loss)+1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwQ5xGFtHiZ0"
      },
      "source": [
        "plt.clf()\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uucaE1pLJBWJ"
      },
      "source": [
        "# 모델을 처음부터 다시 훈련하기\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "print(model.evaluate(x_test, y_test))  # 정확도 87%\n",
        "print(model.predict(x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBh-lxge9X2S"
      },
      "source": [
        "## 3.5 뉴스 기사 분류: 다중 분류 문제\n",
        "3.5.1 로이터 데이터셋\\\n",
        "3.5.2 데이터 준비\\\n",
        "3.5.3 모델 구성\\\n",
        "3.5.4 훈련 검증\\\n",
        "3.5.5 새로운 데이터에 대해 예측하기\\\n",
        "3.5.6 레이블과 손실을 다루는 다른 방법\\\n",
        "3.5.7 충분히 큰 중간층을 두어야 하는 이유\\\n",
        "3.5.8 추가 실험\\\n",
        "3.5.9 정리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne7Dtur29awp"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGDhNuYd99j6"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "import pandas as pd\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)  # 매개변수는 데이터에서 가장 자주 등장하는 단어 1만개로 제한\n",
        "print(len(train_data), len(test_data))\n",
        "print(train_data[0])  # 단어 인덱스 형태\n",
        "print(pd.Series(train_labels).nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhczYg_g_XYL"
      },
      "source": [
        "# 데이터 인코딩(벡터 변환)\n",
        "import numpy as np \n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] =1.  # 부동소숫점 넣어줘야함\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBKbEYqK_7r-"
      },
      "source": [
        "len(x_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu9JWD9SASax"
      },
      "source": [
        "# 원핫 인코딩 방법\n",
        "def to_one_hot(labels, dimension=46):\n",
        "  results = np.zeros((len(labels), dimension))\n",
        "  for i, label in enumerate(labels):\n",
        "    results[i, label] = 1.\n",
        "  return results\n",
        "\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "one_hot_test_labels = to_one_hot(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjIDwTx9BBLk"
      },
      "source": [
        "len(one_hot_train_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTc8lD_FBNHE"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TXx99RiCEEe"
      },
      "source": [
        "# 모델 정의하기\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CFGxcA3EGoI"
      },
      "source": [
        "# 모델 컴파일\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn5_JZfWERXX"
      },
      "source": [
        "# 검증 세트 준비하기\n",
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r_dYvOPEtuX"
      },
      "source": [
        "# 모델 훈련하기\n",
        "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvKbBU4iFt8D"
      },
      "source": [
        "# 훈련과 검증 손실 그리기\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss)+1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYDVlXKhGZ31"
      },
      "source": [
        "# 훈련과 검증 정확도 그리기\n",
        "plt.clf()\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation acc')\n",
        "plt.show()\n",
        "\n",
        "### 9번째 epoch이후에 과대적합. 아홉번의 에포크로 새로운 모델 훈련하고 테스트 세트에서 평가 필요"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUi9V9UZKxPs"
      },
      "source": [
        "# 모델 처음부터 다시 훈련\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpch5IFsL3Zn"
      },
      "source": [
        "predictions = model.predict(x_test)\n",
        "print(predictions[0].shape)\n",
        "print(np.sum(predictions[0]))\n",
        "print(np.argmax(predictions[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArJapJJfN9jl"
      },
      "source": [
        "# 정보 병목이 있는 모델\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(4, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=128, validation_data=(x_val, y_val))\n",
        "\n",
        "### 중간층의 차원이 출력층의 차원보다 작은 경우, 많은 정보를 저차원의 표현 공간으로 압축하려 하기에 정보 손실이 발생함"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI58oFGF7REA"
      },
      "source": [
        "## 3.6 주택 가격 예측: 회귀 문제\n",
        "3.6.1 보스턴 주택 가격 데이터셋\\\n",
        "3.6.2 데이터 준비\\\n",
        "3.6.3 모델 구성\\\n",
        "3.6.4 K-겹 검증을 사용한 훈련 검증\\\n",
        "3.6.5 정리\n",
        "\n",
        "## 3.7 요약"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAmkzLbwIaSJ"
      },
      "source": [
        "3.6.1 보스턴 주택 가격 데이터셋\n",
        "- CRIM - per capita crime rate by town\n",
        "- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "- INDUS - proportion of non-retail business acres per town.\n",
        "- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
        "- NOX - nitric oxides concentration (parts per 10 million)\n",
        "- RM - average number of rooms per dwelling\n",
        "- AGE - proportion of owner-occupied units built prior to 1940\n",
        "- DIS - weighted distances to five Boston employment centres\n",
        "- RAD - index of accessibility to radial highways\n",
        "- TAX - full-value property-tax rate per $10,000\n",
        "- PTRATIO - pupil-teacher ratio by town\n",
        "- B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "- LSTAT - % lower status of the population\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkvBakA_ElDD"
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw7JwDaBG7HY"
      },
      "source": [
        "print(train_data.shape, test_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta9Cp8__HQff"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB06U4w0HnEd"
      },
      "source": [
        "train_targets[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HrUHFJaIjNt"
      },
      "source": [
        "3.6.2 데이터 준비\n",
        "- 피쳐별로 스케일이 다르기에 정규화가 필요함\n",
        "- 각 관측에서 평균을 빼고 표준편차로 나누어 직접 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSL7tj28IkkF"
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "std = train_data.std(axis=0)\n",
        "\n",
        "train_data -= mean\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std  # test 데이터도 train 데이터에서 나온 mean, std로 표준화 해줘야함\n",
        "\n",
        "print(train_data[0])\n",
        "print(test_data[0])\n",
        "\n",
        "### test 데이터를 정규화할때 사용한 값이 훈련 데이터에서 계산한 값이어야 함\n",
        "### 머신러닝 작업과정에서 절대로 test 데이터에서 사용한 어떤 값도 사용하면 안됨\n",
        "### train, test를 다른 스케일로 변환하게 되면 train에서 학습한 정보가 쓸모없게 됨"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3kskWbpKSN1"
      },
      "source": [
        "3.6.3 모델 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlInh0w-JLBO"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import metrics\n",
        "\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  # 모델의 훈련 데이터가 적기에, 과대적합 피하기 위해 은닉층의 수가 적은 모델(1~2개)이 좋음\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "  # model.compile(optimizer='rmsprop', loss='mse', metrics=[metrics.MeanAbsoluteError()])\n",
        "\n",
        "  # 회귀이기에 손실함수:mse, 평가지표:mae\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2vsn-MbLxuz"
      },
      "source": [
        "3.6.4 K-겹 검증을 사용한 훈련 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2ZiVJ1ELzCv"
      },
      "source": [
        "# K-fold 검증하기\n",
        "import numpy as np\n",
        "k=4\n",
        "num_val_samples = len(train_data) // k  # k로 나눈 몫만 사용\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "\n",
        "for i in range(k):\n",
        "  print('처리중인 폴드 #',i)\n",
        "  val_data = train_data[i*num_val_samples: (i+1)*num_val_samples]\n",
        "  val_targets = train_targets[i*num_val_samples: (i+1)*num_val_samples]\n",
        "\n",
        "  partial_train_data = np.concatenate(\n",
        "      [train_data[:i*num_val_samples],\n",
        "       train_data[(i+1)*num_val_samples:]],axis=0)\n",
        "  partial_train_targets = np.concatenate(\n",
        "      [train_targets[:i*num_val_samples],\n",
        "       train_targets[(i+1)*num_val_samples:]], axis=0)\n",
        "  \n",
        "  model = build_model()\n",
        "  model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "  all_scores.append(val_mae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxN0sHx9RlmZ"
      },
      "source": [
        "print(all_scores)\n",
        "print(np.mean(all_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnFQmRyuSOBW"
      },
      "source": [
        "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9E0oKObMIXF"
      },
      "source": [
        "# 각 폴드에서 검증 점수를 로그에 저장하기\n",
        "num_epochs =  500\n",
        "all_mae_histories = []\n",
        "\n",
        "for i in range(k):\n",
        "  print('처리중인 폴드 #',i)\n",
        "  val_data = train_data[i*num_val_samples:(i+1)*num_val_samples]\n",
        "  val_targets = train_targets[i*num_val_samples: (i+1)*num_val_samples]\n",
        "\n",
        "  partial_train_data = np.concatenate(\n",
        "      [train_data[:i*num_val_samples],\n",
        "       train_data[(i+1)*num_val_samples:]], axis=0)\n",
        "  partial_train_targets = np.concatenate(\n",
        "      [train_targets[:i*num_val_samples],\n",
        "       train_targets[(i+1)*num_val_samples:]], axis=0)\n",
        "  \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, partial_train_targets,\n",
        "                      validation_data=(val_data, val_targets),\n",
        "                      epochs=num_epochs, batch_size=1, verbose=0)\n",
        "  mae_history = history.history['val_mae']\n",
        "  all_mae_histories.append(mae_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cdS3w9RbfjU"
      },
      "source": [
        "# K-fold 검증 점수 평균 기록\n",
        "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qer9cZB3bzBR"
      },
      "source": [
        "# 검증 점수 그래프\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, len(average_mae_history)+1), average_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validataion MAE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q63EyxcIcbVf"
      },
      "source": [
        "# 처음 10개 데이터 제외한 검증 점수 그래프 그리기\n",
        "\n",
        "def smooth_curve(points, factor=0.9):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous*factor + point*(1-factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
        "\n",
        "plt.plot(range(1,len(smooth_mae_history)+1), smooth_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dLjqgLKdgL-"
      },
      "source": [
        "# 최종 모델 훈련하기\n",
        "\n",
        "model = build_model()\n",
        "model.fit(train_data, train_targets, epochs=80, batch_size=16, verbose=0)\n",
        "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsjaWBAim_qT"
      },
      "source": [
        ""
      ]
    }
  ]
}